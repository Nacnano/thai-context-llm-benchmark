{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction and Grading\n",
    "\n",
    "### Prompt\n",
    "\n",
    "```\n",
    "I have questions generated by visual genome data and template questions. Some of them are still wrong. Can you correct them and grade them to score from 1 to 5 using criterias (Grammatical,Relevance,Clarity,Specificity,Prettiness,Average Score,Reason for Corrections)?\n",
    "\n",
    "Please return the output as CSV file with\n",
    "Original Question,Corrected Question,Grammatical,Relevance,Clarity,Specificity,Prettiness,Average Score,Reason for Corrections as headers\n",
    "\n",
    "Here is the generated questions along with its context data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and Standard Deviation for each category:\n",
      "Grammatical: LLM Mean = 4.667, LLM Std = 0.478, Human Mean = 5.000, Human Std = 0.000\n",
      "Relevance: LLM Mean = 5.000, LLM Std = 0.000, Human Mean = 4.778, Human Std = 0.929\n",
      "Clarity: LLM Mean = 5.000, LLM Std = 0.000, Human Mean = 4.778, Human Std = 0.929\n",
      "Specificity: LLM Mean = 5.000, LLM Std = 0.000, Human Mean = 4.750, Human Std = 0.937\n",
      "Prettiness: LLM Mean = 4.667, LLM Std = 0.478, Human Mean = 4.889, Human Std = 0.667\n",
      "\n",
      "T-tests (t-statistic, p-value) and Hypothesis Decision:\n",
      "Grammatical: t-statistic = -0.000, p-value = 1.000 => Don't reject H_0\n",
      "Relevance: t-statistic = -0.000, p-value = 1.000 => Don't reject H_0\n",
      "Clarity: t-statistic = -0.000, p-value = 1.000 => Don't reject H_0\n",
      "Specificity: t-statistic = 0.000, p-value = 1.000 => Don't reject H_0\n",
      "Prettiness: t-statistic = -0.000, p-value = 1.000 => Don't reject H_0\n",
      "\n",
      "Mean Absolute Error (MAE):\n",
      "Grammatical: 0.333\n",
      "Relevance: 0.222\n",
      "Clarity: 0.222\n",
      "Specificity: 0.250\n",
      "Prettiness: 0.444\n",
      "\n",
      "Root Mean Squared Error (RMSE):\n",
      "Grammatical: 0.577\n",
      "Relevance: 0.943\n",
      "Clarity: 0.943\n",
      "Specificity: 0.957\n",
      "Prettiness: 0.882\n",
      "\n",
      "Recall:\n",
      "Grammatical: 0.000\n",
      "Relevance: 0.000\n",
      "Clarity: 0.000\n",
      "Specificity: 0.000\n",
      "Prettiness: 0.000\n",
      "\n",
      "Precision:\n",
      "Grammatical: 0.000\n",
      "Relevance: 0.000\n",
      "Clarity: 0.000\n",
      "Specificity: 0.000\n",
      "Prettiness: 0.000\n",
      "\n",
      "Accuracy:\n",
      "Grammatical: 0.667\n",
      "Relevance: 0.944\n",
      "Clarity: 0.944\n",
      "Specificity: 0.917\n",
      "Prettiness: 0.639\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Root directory for your data\n",
    "root_directory = \"data/corrected/\"\n",
    "df_llm = pd.read_csv(root_directory + 'gpt3-5_graded_corrected_questions.csv')\n",
    "df_human = pd.read_csv(root_directory + 'manually_graded_corrected_questions.csv')\n",
    "\n",
    "# List of categories to compare\n",
    "categories = [\"Grammatical\", \"Relevance\", \"Clarity\", \"Specificity\", \"Prettiness\"]\n",
    "\n",
    "t_tests = {}\n",
    "hypothesis_results = {}\n",
    "\n",
    "mae_scores = {}\n",
    "rmse_scores = {}\n",
    "recall_scores = {}\n",
    "precision_scores = {}\n",
    "accuracy_scores = {}\n",
    "\n",
    "# Calculating mean and std for each category for both LLM and Human scores\n",
    "means_std = {}\n",
    "\n",
    "for category in categories:\n",
    "    llm_scores = df_llm[category]\n",
    "    human_scores = df_human[category]\n",
    "    \n",
    "    # Calculate mean and std for LLM\n",
    "    llm_mean = llm_scores.mean()\n",
    "    llm_std = llm_scores.std()\n",
    "\n",
    "    # Calculate mean and std for Human\n",
    "    human_mean = human_scores.mean()\n",
    "    human_std = human_scores.std()\n",
    "\n",
    "    # Store mean and std in a dictionary\n",
    "    means_std[category] = {\n",
    "        'LLM Mean': llm_mean,\n",
    "        'LLM Std': llm_std,\n",
    "        'Human Mean': human_mean,\n",
    "        'Human Std': human_std\n",
    "    }\n",
    "\n",
    "    # Z-score normalization (handling std = 0)\n",
    "    if llm_std == 0:\n",
    "        llm_zscores = np.zeros_like(llm_scores)\n",
    "    else:\n",
    "        llm_zscores = (llm_scores - llm_mean) / llm_std\n",
    "\n",
    "    if human_std == 0:\n",
    "        human_zscores = np.zeros_like(human_scores)\n",
    "    else:\n",
    "        human_zscores = (human_scores - human_mean) / human_std\n",
    "\n",
    "    # Perform paired t-test on z-scores\n",
    "    t_stat, p_value = ttest_rel(llm_zscores, human_zscores)\n",
    "    t_tests[category] = (t_stat, p_value)\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        hypothesis_results[category] = \"Reject H_0\"\n",
    "    else:\n",
    "        hypothesis_results[category] = \"Don't reject H_0\"\n",
    "    \n",
    "    # Calculate MAE and RMSE (on original scores, not z-scores)\n",
    "    mae = mean_absolute_error(human_scores, llm_scores)\n",
    "    rmse = np.sqrt(mean_squared_error(human_scores, llm_scores))\n",
    "    mae_scores[category] = mae\n",
    "    rmse_scores[category] = rmse\n",
    "\n",
    "    # Adjusted logic for recall, precision, and accuracy\n",
    "    # Correct predictions: LLM < threshold and Human < threshold (for positive cases)\n",
    "    negative_threshold = 5  # Invert the threshold logic\n",
    "\n",
    "    # Calculate true positives, false negatives, false positives, true negatives\n",
    "    true_positives = np.sum((llm_scores < negative_threshold) & (human_scores < negative_threshold))\n",
    "    false_negatives = np.sum((llm_scores >= negative_threshold) & (human_scores < negative_threshold))\n",
    "    false_positives = np.sum((llm_scores < negative_threshold) & (human_scores >= negative_threshold))\n",
    "    true_negatives = np.sum((llm_scores >= negative_threshold) & (human_scores >= negative_threshold))\n",
    "\n",
    "    # Recall, Precision, Accuracy\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    accuracy = (true_positives + true_negatives) / (len(llm_scores))  # Total correct predictions / total predictions\n",
    "\n",
    "    recall_scores[category] = recall\n",
    "    precision_scores[category] = precision\n",
    "    accuracy_scores[category] = accuracy\n",
    "\n",
    "# Output mean and std results\n",
    "print(\"Mean and Standard Deviation for each category:\")\n",
    "for category, stats in means_std.items():\n",
    "    print(f\"{category}: LLM Mean = {stats['LLM Mean']:.3f}, LLM Std = {stats['LLM Std']:.3f}, \"\n",
    "          f\"Human Mean = {stats['Human Mean']:.3f}, Human Std = {stats['Human Std']:.3f}\")\n",
    "\n",
    "# Output t-test results\n",
    "print(\"\\nT-tests (t-statistic, p-value) and Hypothesis Decision:\")\n",
    "for category, result in t_tests.items():\n",
    "    t_stat, p_value = result\n",
    "    decision = hypothesis_results[category]\n",
    "    print(f\"{category}: t-statistic = {t_stat:.3f}, p-value = {p_value:.3f} => {decision}\")\n",
    "\n",
    "# Output Mean Absolute Error results\n",
    "print(\"\\nMean Absolute Error (MAE):\")\n",
    "for category, mae in mae_scores.items():\n",
    "    print(f\"{category}: {mae:.3f}\")\n",
    "\n",
    "# Output Root Mean Squared Error results\n",
    "print(\"\\nRoot Mean Squared Error (RMSE):\")\n",
    "for category, rmse in rmse_scores.items():\n",
    "    print(f\"{category}: {rmse:.3f}\")\n",
    "\n",
    "# Output Recall results\n",
    "print(\"\\nRecall:\")\n",
    "for category, recall in recall_scores.items():\n",
    "    print(f\"{category}: {recall:.3f}\")\n",
    "\n",
    "# Output Precision results\n",
    "print(\"\\nPrecision:\")\n",
    "for category, precision in precision_scores.items():\n",
    "    print(f\"{category}: {precision:.3f}\")\n",
    "\n",
    "# Output Accuracy results\n",
    "print(\"\\nAccuracy:\")\n",
    "for category, accuracy in accuracy_scores.items():\n",
    "    print(f\"{category}: {accuracy:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
